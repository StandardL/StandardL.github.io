---
title: Python爬虫0.5 - 常用库的介绍
date: 2023-07-03 16:05:34 +0800
categories: [Python, 爬虫]
tags: [Python, 爬虫]      # TAG names should always be lowercase
---

## Requests

### 功能

Python requests 是一个常用的 HTTP 请求库，可以方便地向网站发送 HTTP 请求，并获取响应结果。

requests 模块比 [urllib](https://www.runoob.com/python3/python-urllib.html) 模块更简洁。

使用 requests 发送 HTTP 请求需要先导入 requests 模块.

![image-20230703160656897](https://github.com/StandardL/StandardL.github.io/raw/main/assets/img/posts/2023-07-03-Python%E7%88%AC%E8%99%AB0.5/Requests%E6%B5%81%E7%A8%8B.png)

### 使用

#### 发送请求

requests中发送请求的代码段为：

```python
request.get/post(url, parms, data, headers, timeout, verify, allow_redirects, cookies)
```

在客户机和服务器之间进行请求-响应时，两种最常被用到的方法是：GET 和 POST。

- **GET** - 从指定的资源请求数据。
- **POST** - 向指定的资源提交要被处理的数据。

GET 提交参数一般显示在 URL 上，POST 通过表单提交不会显示在 URL 上，POST 更具隐蔽性.

有关GET和POST的区别，请参阅：[GET VS POST](https://www.runoob.com/tags/html-httpmethods.html)；[GET 和 POST请求的本质区别是什么](https://cloud.tencent.com/developer/article/1915518)

其他参数：

- **url**：目标网页的网址。

- params：字典形式，设置URL后面的参数，如?id=123&name=zhangsan。

- **data**：字典或者字符串，用于**POST**方法提交数据。

- **headers**：设置请求头，如user-agent，refer等。

  - user-agent例子：以谷歌Chrome浏览器114.0.5735.119为例。

    Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36

    关于user-agent为啥这么长，以后有机会会新开一篇文章简单讲解一下。

- **timeout**：超时时间，单位为秒。

- verify: True/ False，是否进行HTTPS证书验证，默认为是，需要自己设置证书地址。

- allow-redirects: True/ False，是否允许链接重定向，默认是。

- cookies：附带本地的cookies缓存数据。比如你的登录状态，记住的密码等等。

#### 接受响应

接受response响应：

```python
r = requests.get/post(url)

r.status_code  # 查看返回的状态码，200表示成功
r.encoding  # 查看当前网页的编码格式，也可用于修改编码。默认会根据获取的headers推测编码，若找不到则默认为ISO-8859-1编码
r.text  # 查看返回的网页内容
r.headers  # 查看返回的HTTP的headers
r.url  # 查看实际访问的url（这个是有可能与请求的url是不同的）
r.content  # 以字节的方式返回内容，比如用于下载图片
r.cookies  # 写入的本地cookies数据
```

更多关于requests的内容，请访问：[菜鸟教程](https://www.runoob.com/python3/python-requests.html)，[Pypi](https://pypi.org/project/requests/)



## URL管理器

### 功能

对爬取URL进行管理，防止重复和循环爬取，并支持新增URL和取出URL。

1. 使用set来分别存储未爬取和已爬取的URL
2. 若是待爬取数量过多，可以使用**redis**进行存储
3. 也可以使用**SQL**（如MySQL）的一张表**urls**进行存储，表内有两列，一是url，二是爬取状态is_crawled

### 实现

请参阅新的文章。