---
title: Python爬虫2.1 - 爬虫实战1 - 爬取自己的Blog
date: 2023-07-17 17:08:15 +0800
categories: [Python, 爬虫]
tags: [Python, 爬虫]      # TAG names should always be lowercase
---

# 爬虫实战1 - 爬取自己的Blog

前面我们学习了很多关于Python爬虫的基础知识，现在我们就来实战一下，以我个人的blog作为第一个爬虫网站爬取。

## 爬取目标

我们这次的爬取的目标是blog上每篇文章的链接以及对应的标题，我们可以先探索一下我们的blog。

![image-20230717171244909](https://github.com/StandardL/StandardL.github.io/raw/main/assets/img/posts/2023-07-17-Python爬虫2.1/探索blog.png)

可以看到，我们的标题是在一个`h1`的标签下的，并带有一个超链接。

## 开始爬取

打开PyCharm，然后在我们的项目根目录下新建一个Python软件包，命名为blog_test，然后在该文件夹内新建一个Python文件，命名为main，最终得到的结构如下：

![image-20230717171527048](https://github.com/StandardL/StandardL.github.io/raw/main/assets/img/posts/2023-07-17-Python爬虫2.1/文件结构.png)

然后在main.py文件内，输入以下代码：

```python
from bs4 import BeautifulSoup
import requests

url = "https://standardl.github.io/"

r = requests.get(url)
if r.status_code != 200:
    raise Exception()

html_doc = r.text
soup = BeautifulSoup(html_doc, 'html.parser')

h1_nodes = soup.find_all("h1")
for h1_node in h1_nodes:
    link = h1_node.find("a")
    print(link["href"], link.get_text())

```

下面简单的解释一下我们的代码：

这里头两行是导入包，按照规范，我们一般把包的导入放在文件首，便于其他用户安装包。

url变量指明了我们需要抓包的网站。

第6行的作用是爬取我们需要的网页HTML，第7-8行的作用是确保我们是正确从服务器中获取了对应的网页（如有其他错误，status_code是不为200的）。

第10-11行的作用是解析我们的HTML文本。

第13行的作用是根据我们探索网页的结果，按需查找的节点。

第14-16行的作用是输出我们爬取到的所有文本内容。

## 输出结果

由于我的blog会一直更新，因此爬虫结果和我下面得出来的结果不会是一模一样的。

```
/posts/Python%E7%88%AC%E8%99%AB1.4-%E6%8E%A2%E7%B4%A2%E5%BE%85%E7%88%AC%E5%8F%96%E7%9A%84%E7%BD%91%E9%A1%B5/ Python爬虫1.4 - 探索待爬取的网页
/posts/Python%E7%88%AC%E8%99%AB1.3-Beautiful-Soup%E7%9A%84%E4%BD%BF%E7%94%A8/ Python爬虫1.3 - Beautiful Soup的使用
/posts/Python%E7%88%AC%E8%99%AB1.2-URL%E7%AE%A1%E7%90%86%E5%99%A8/ Python爬虫1.2 - URL管理器
/posts/Python%E7%88%AC%E8%99%AB1.1-Requests%E7%9A%84%E4%BD%BF%E7%94%A8/ Python爬虫1.1 - Requests的使用
/posts/Python%E7%88%AC%E8%99%AB0.5-%E5%90%84%E4%B8%AA%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BB%8B%E7%BB%8D/ Python爬虫0.5 - 各个模块的介绍
/posts/Python%E7%88%AC%E8%99%AB0-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/ Python爬虫0 - 环境准备
/posts/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%E6%8F%90%E7%BA%B2-%E8%A1%A5%E5%85%85/ 数据库期末复习提纲
```

可见，当前我blog上的所有文章都被正确爬取了。