---
title: Python爬虫0.5 - 各个模块的介绍
date: 2023-07-03 16:05:34 +0800
categories: [Python, 爬虫]
tags: [Python, 爬虫]      # TAG names should always be lowercase
---

## Requests

### 功能

Python requests 是一个常用的 HTTP 请求库，可以方便地向网站发送 HTTP 请求，并获取响应结果。

requests 模块比 [urllib](https://www.runoob.com/python3/python-urllib.html) 模块更简洁。

使用 requests 发送 HTTP 请求需要先导入 requests 模块.

![image-20230703160656897](https://github.com/StandardL/StandardL.github.io/raw/main/assets/img/posts/2023-07-03-Python%E7%88%AC%E8%99%AB0.5/Requests%E6%B5%81%E7%A8%8B.png)

### 使用

#### 发送请求

requests中发送请求的代码段为：

```python
request.get/post(url, parms, data, headers, timeout, verify, allow_redirects, cookies)
```

在客户机和服务器之间进行请求-响应时，两种最常被用到的方法是：GET 和 POST。

- **GET** - 从指定的资源请求数据。
- **POST** - 向指定的资源提交要被处理的数据。

GET 提交参数一般显示在 URL 上，POST 通过表单提交不会显示在 URL 上，POST 更具隐蔽性.

有关GET和POST的区别，请参阅：[GET VS POST](https://www.runoob.com/tags/html-httpmethods.html)；[GET 和 POST请求的本质区别是什么](https://cloud.tencent.com/developer/article/1915518)

其他参数：

- **url**：目标网页的网址。

- params：字典形式，设置URL后面的参数，如?id=123&name=zhangsan。

- **data**：字典或者字符串，用于**POST**方法提交数据。

- **headers**：设置请求头，如user-agent，refer等。

  - user-agent例子：以谷歌Chrome浏览器114.0.5735.119为例。

    Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36

    关于user-agent为啥这么长，以后有机会会新开一篇文章简单讲解一下。

- **timeout**：超时时间，单位为秒。

- verify: True/ False，是否进行HTTPS证书验证，默认为是，需要自己设置证书地址。

- allow-redirects: True/ False，是否允许链接重定向，默认是。

- cookies：附带本地的cookies缓存数据。比如你的登录状态，记住的密码等等。

#### 接受响应

接受response响应：

```python
r = requests.get/post(url)

r.status_code  # 查看返回的状态码，200表示成功
r.encoding  # 查看当前网页的编码格式，也可用于修改编码。默认会根据获取的headers推测编码，若找不到则默认为ISO-8859-1编码
r.text  # 查看返回的网页内容
r.headers  # 查看返回的HTTP的headers
r.url  # 查看实际访问的url（这个是有可能与请求的url是不同的）
r.content  # 以字节的方式返回内容，比如用于下载图片
r.cookies  # 写入的本地cookies数据
```

补充：requests请求时附带cookie字典

```python
import requests
cookie = {
    "BAIDUID_BFESS" : "4051D8C947D33A9ED35222FECFF4AEB9:FG=1"
}
r = requests.get(
	"http://url", 
    cookies = cookie
)
```

更多关于requests的内容，请访问：[菜鸟教程 - requests](https://www.runoob.com/python3/python-requests.html)，[Pypi](https://pypi.org/project/requests/)

---

## URL管理器

### 功能

对爬取URL进行管理，防止重复和循环爬取，并支持新增URL和取出URL。

1. 使用set来分别存储未爬取和已爬取的URL
2. 若是待爬取数量过多，可以使用**redis**进行存储
3. 也可以使用**SQL**（如MySQL）的一张表**urls**进行存储，表内有两列，一是url，二是爬取状态is_crawled

### 实现

请参阅[URL管理器的实现 - 使用Python内存](https://standardl.github.io/posts/Python%E7%88%AC%E8%99%AB1.2-URL%E7%AE%A1%E7%90%86%E5%99%A8/)

---

## HTML语言

由于我们的requests模块抓取的数据结果都是HTML语言，因此我们需要对HTML语言有一定的了解。由于Jekyll框架对HTML的支持不算很好，因此这里不展开介绍，需要了解关于HTML的内容，请访问：[菜鸟教程 - HTML](https://www.runoob.com/html/html-tutorial.html)，[w3school](https://www.w3school.com.cn/)

---

## 网页解析器 - beautifulsoup

### 功能

解析HTML代码，获取我们需要的信息。

官方中文文档：[Beautiful Soup 中文文档](https://beautifulsoup.cn/)

![image-20230709231035000](https://github.com/StandardL/StandardL.github.io/raw/main/assets/img/posts/2023-07-03-Python%E7%88%AC%E8%99%AB0.5/BeautifulSoup语法.png)

其中，节点处，我们一般访问的是属性和文字，属性对应HTML标签中的属性，比如href指向的超链接；而文字则是HTML中的文本文字，也就是我们提取出来的诸如“百度一下，你就知道”的文字。

### 实现

#### 初始化（创建对象）

```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(
			html_doc,  # 待解析的HTML文档字符串
			'html.parser',  # HTML解析器（默认即可）
			from_encoding='utf8'  # HTML文档的编码格式，可以通过requests进行预处理，此项因此不是必须项
			)
```

#### 搜索节点

```python
soup.find_all('a')  # 查找所有标签为a的节点
soup.find_all('a', href='niganma/hahaaiyo/233.html')  # 查找所有标签为a且链接符合href形式的节点
soup.find_all('a', class_='abc', string='Python')  # 查找所有标签为a且class为abc，文字为Python的节点
```

这里，有 find_all 和 find 方法两种方式，他们的唯一区别是 find_all 会以列表形式返回所有符合搜索条件的结果，而 find 只会返回第一个符合搜索条件的结果。

#### 访问节点信息

```python
node.nmae  # 获取查找到的节点的标签名
node['href']  # 获取找到的节点的href属性
node.get_text()  # 获取找到的节点的链接文字
```

---

## Python使用正则表达式实现模糊匹配

```python
import re
pattern = r'^https://standardl.github.io/posts/\S+$'

print(re.match(pattern, url)) 
# 当匹配成功时，其返回一个一赔的对象；否则返回None
```

更多关于Python正则表达式，请访问：[菜鸟教程 - Python正则表达式](https://www.runoob.com/python/python-reg-expressions.html)，重点关注正则表达式模式部分。
